同时 我们 还要 准备 词 向量 模型 ， 就是 能够 把 每个 中文 词 映射 到 一个 向量 上 ， 训练 这个 词 向量 模型 ， 是 一个 需要 大量 语料 的 工作 ， 通过观察 词语 在 句子 中 的 上下文 的 位置 ， 从而 确定 词语 本身 的 向量 。 非常 遗憾 我大 中国 ， 没有 什么 可以 使用 的 开放 的 中文 语料 （ 可悲 ， 可叹 ， 可怜 ， 国家 的 钱 都 白花 了 ） ， 我们 使用 了 中文   wiki   提供 的 “ 国际 援助 ” 。 把 对话 内容 进行 分词 ， 然后 通过 加载 词 向量 模型 找到 每个 词 的 向量 ， 如果 找 不到 就 用 一个 默认 向量 替代 （ 惨 ， 说明 词 向量 模型 覆盖 的 不够 ） ， 然后 每个 会话 就 构成 了 一个二维 矩阵 ， 每行 是 一个 词 向量 ， 多少 行 就是 这个 会话 中 有 多少 词 ， 就是 所谓 的 时间 序列 ， 因为 话 是从 前往 后 说 。 （ 有 倒 着 说话 的 吗 ？ 倒背如流 ） 这里 使用   GRU ， 相当于 简化 的   LSTM   吧 ， 广义 上 都 是   RNN （ 自己 研究 吧 ， 真 没 啥 东西 ， 我们 只 讨论 如何 正确 的 使用 ） ； 一个   RNN   网络 实际上 就是 能够 把 时间 序列 数据 一个个 输入 ， 这里 就是 一个个 词 向量 进行 输入 （ 再 加上 上下文 数据 ） ， 然后   RNN   网络 输出 两个 值 ， 一个 是 正常 的 输出 ， 一个 是 上下文 输出 。 这 里面 非常 重要 的 信息 是 ， RNN   输入 一定 包含 两 部分 ， 一部分 是 时间 序列 中 的 一个 元素 ； 另 一部分 是 上下文 。 每次   RNN   需要 的 输入 只是 时间 序列 的 一个 元素 ， 不是 整个 序列 ， RNN   可 没有 这么 牛 逼 （ 很多 讲   RNN   的 示例 图 都 画 得 好像   RNN   能 一次 吞 了 整个 时间 序列 数据 ， 给 人 无限 困扰 ） ， 但是 我们 在 使用   PyTorch   的 时候 输入 数据 是 整个 序列 ， 那 是 程序 抽象 出来 的 方便使用 而已 。 简 书简 书 著作权 归 作者 所有 ， 任何 形式 的 转载 都 请 联系 作者 获得 授权 并 注明 出处 。