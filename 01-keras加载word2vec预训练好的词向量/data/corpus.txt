同时我们还要准备词向量模型，就是能够把每个中文词映射到一个向量上，训练这个词向量模型，是一个需要大量语料的工作，通过观察词语在句子中的上下文的位置，从而确定词语本身的向量。非常遗憾我大中国，没有什么可以使用的开放的中文语料（可悲，可叹，可怜，国家的钱都白花了），我们使用了中文 wiki 提供的“国际援助”。
把对话内容进行分词，然后通过加载词向量模型找到每个词的向量，如果找不到就用一个默认向量替代（惨，说明词向量模型覆盖的不够），然后每个会话就构成了一个二维矩阵，每行是一个词向量，多少行就是这个会话中有多少词，就是所谓的时间序列，因为话是从前往后说。（有倒着说话的吗？倒背如流）
这里使用 GRU，相当于简化的 LSTM 吧，广义上都是 RNN（自己研究吧，真没啥东西，我们只讨论如何正确的使用）；一个 RNN 网络实际上就是能够把时间序列数据一个个输入，这里就是一个个词向量进行输入（再加上上下文数据），然后 RNN 网络输出两个值，一个是正常的输出，一个是上下文输出。这里面非常重要的信息是，RNN 输入一定包含两部分，一部分是时间序列中的一个元素；另一部分是上下文。每次 RNN 需要的输入只是时间序列的一个元素，不是整个序列，RNN 可没有这么牛逼（很多讲 RNN 的示例图都画得好像 RNN 能一次吞了整个时间序列数据，给人无限困扰），但是我们在使用 PyTorch 的时候输入数据是整个序列，那是程序抽象出来的方便使用而已。简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。